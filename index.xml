<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>A Hugo website</title>
    <link>/</link>
    <description>Recent content on A Hugo website</description>
    <generator>Hugo</generator>
    <language>en-US</language>
    <lastBuildDate>Sun, 16 Feb 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Genome-wide association studies</title>
      <link>/post/gwas/</link>
      <pubDate>Sun, 16 Feb 2025 00:00:00 +0000</pubDate>
      <guid>/post/gwas/</guid>
      <description>&lt;h2 id=&#34;variants-trait-association&#34;&gt;Variants-trait association&lt;/h2&gt;&#xA;&lt;p&gt;The core objective of genetic studies is to identify which genetic variants contribute to disease risk. While establishing direct causation is challenging, we can detect statistical associations between genetic variants and traits by analyzing large-scale genomic data.&lt;/p&gt;&#xA;&lt;p&gt;Large biobanks (such as the UK Biobank) have collected genomic data from hundreds of thousands of samples. To study variant-trait associations, one approach is to apply linear or logistic regression for each genetic variant, treating genotypes as independent variables and the trait as the dependent variable.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Linkage Disequilibrium Score Regression</title>
      <link>/post/ldsc/</link>
      <pubDate>Sun, 05 Jan 2025 00:00:00 +0000</pubDate>
      <guid>/post/ldsc/</guid>
      <description>&lt;h2 id=&#34;ldsc-derivation&#34;&gt;LDSC derivation&lt;/h2&gt;&#xA;&lt;p&gt;We discussed how to perform &lt;a href=&#34;/post/gwas/&#34;&gt;GWAS with scaled genotypes &amp;amp; phenotype&lt;/a&gt;. In this blog post, I present an important piece of result: Linkage Disequilibrium Score Regression (LDSC)&lt;/p&gt;&#xA;&lt;p&gt;LDSC was proposed in &lt;a href=&#34;https://www.nature.com/articles/ng.3211&#34;&gt;this&lt;/a&gt; landmark paper, in which it described how LD affect the probability of a variant being significant. Under infinitesimal model, LDSC states &lt;code&gt;\(\mathbb{E}[\chi_j^2] = \frac{Nh^2}{M} l_j + 1\)&lt;/code&gt;, where &lt;code&gt;\(l_j \equiv \sum_{k = 1}^M r_{jk}^2\)&lt;/code&gt; is the LD score. To carry out the derivation, one must treat the effect size as random: &lt;code&gt;\(\lambda_j \sim N(0, \frac{h^2}{M})\)&lt;/code&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Hidden Markov Model (1) - Markov Chain</title>
      <link>/post/hmm1/</link>
      <pubDate>Mon, 29 Nov 2021 00:00:00 +0000</pubDate>
      <guid>/post/hmm1/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;&#xA;&lt;p&gt;This series of blog posts aims to explore the Hidden Markov Model (HMM) due to its broad applications across various fields, including natural language processing, population genetics, finance, and more. Beyond its practical utility, I find HMM particularly fascinating because it bridges multiple disciplines such as probability, linear algebra, machine learning, and computer science. In this post, I will introduce the Markov Chain, which serves as the foundation of HMM. As before, the concepts will be explained through a simple example, with minimal use of complex mathematical notation.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Hidden Markov Model (2) - Forward Backward Propagation</title>
      <link>/post/hmm2/</link>
      <pubDate>Mon, 29 Nov 2021 00:00:00 +0000</pubDate>
      <guid>/post/hmm2/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;&#xA;&lt;p&gt;This post will include a few sections:&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Introducing HMM, and demonstrate how it different from the Markov Chain&lt;/li&gt;&#xA;&lt;li&gt;Introducing an exhaustive method to infer the hidden state&lt;/li&gt;&#xA;&lt;li&gt;Introducing forward-backward propagation as an improvement&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;The example is from &lt;a href=&#34;https://www.youtube.com/watch?v=VBs8FYsZIN4&#34;&gt;Dr.Xiaole Liu&amp;rsquo;s Youtube channel&lt;/a&gt;, and I highly recommend you to check out her video if you want to develop intuition of HMM rather than get killed by notations. Also, you may want to review &lt;a href=&#34;https://en.wikipedia.org/wiki/Conditional_independence&#34;&gt;conditional independence&lt;/a&gt; before you start reading, since it will be very frequently used later in this post.&lt;/p&gt;</description>
    </item>
    <item>
      <title>An Intuitive Explanation of Bayesian Network</title>
      <link>/post/bayesian_network/</link>
      <pubDate>Sat, 27 Nov 2021 15:19:26 -0600</pubDate>
      <guid>/post/bayesian_network/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;&#xA;&lt;p&gt;Bayesian network, a probabilistic model that represents the causal relationship between variables, has gain its popularity in various fields. In biology, for example, people start to use this model to infer genetic regulatory network (GRN) due to its nice property of being directional. The aim of this blog post is to provide a gentle and less-mathematical introduction to Bayesian network.&lt;/p&gt;&#xA;&lt;p&gt; &lt;br&gt;&#xA; &lt;br&gt;&#xA; &lt;br&gt;&#xA; &lt;/p&gt;&#xA;&lt;h2 id=&#34;an-example&#34;&gt;An example&lt;/h2&gt;&#xA;&lt;p&gt;Suppose we are going to take a math exam next week. The outcome of the exam heavily depends on two factors: &lt;strong&gt;sleep&lt;/strong&gt; and &lt;strong&gt;study&lt;/strong&gt;. If we study and sleep well, chances are high that we will do a good job in the exam. Also, &lt;strong&gt;sleep&lt;/strong&gt; can affect our attention and therefore influence our &lt;strong&gt;study&lt;/strong&gt; quality. Since the world is probabilistic, we need to define the probability of each action (&lt;strong&gt;sleep&lt;/strong&gt;, &lt;strong&gt;study&lt;/strong&gt; and &lt;strong&gt;exam&lt;/strong&gt;):&lt;/p&gt;</description>
    </item>
    <item>
      <title>Model the Gene Expression (2): Likelihood Ratio Test</title>
      <link>/post/gene_exp2/</link>
      <pubDate>Tue, 23 Nov 2021 15:19:26 -0600</pubDate>
      <guid>/post/gene_exp2/</guid>
      <description>&lt;p&gt;In the &lt;a href=&#34;/post/gene_exp1/&#34;&gt;last post&lt;/a&gt;, we used a GLM framework to model the gene expression.&#xA;$$&#xA;y \sim NB(\mu, r) \\&#xA;log( \mu )= b_1 x + b_0&#xA;$$&lt;/p&gt;&#xA;&lt;p&gt;Using &lt;a href=&#34;/post/mle/&#34;&gt;maximum likelihood estimation&lt;/a&gt;, we were able to find a set of parameters &lt;code&gt;\(\hat b_0, \hat b_1, \hat r\)&lt;/code&gt;, that maximizes the likelihood function.&lt;/p&gt;&#xA;&lt;p&gt;But if you send this model (the estimated parameters) to biologists, they wouldn&amp;rsquo;t be happy. And we all know what is lacking: &lt;strong&gt;the p-value!&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Model the Gene Expression (1): A GLM framework</title>
      <link>/post/gene_exp1/</link>
      <pubDate>Tue, 23 Nov 2021 15:18:26 -0600</pubDate>
      <guid>/post/gene_exp1/</guid>
      <description>&lt;p&gt;Before you start reading this post, please familiarize yourself with &lt;a href=&#34;/post/mle/&#34;&gt;MLE&lt;/a&gt; and linear model.&lt;/p&gt;&#xA;&lt;p&gt; &lt;br&gt;&#xA; &lt;/p&gt;&#xA;&lt;h2 id=&#34;background&#34;&gt;Background&lt;/h2&gt;&#xA;&lt;p&gt;In transcriptomic research, we often want to determine if genes are unregulated or down-regulated under a particular perturbation. For example, we have a medication that may cure type 2 diabetes. In our experiment, 6 patients are split into two groups, with 3 patients taking the medication, and 3 patients taking the placebo. The patients&amp;rsquo; blood samples are then collected to measure the transcriptomic profile (mRNA abundance level for each gene) using NGS technology (&lt;a href=&#34;https://en.wikipedia.org/wiki/RNA-Seq&#34;&gt;RNA seq&lt;/a&gt;). The mRNA abundance levels are quantified by the number of &lt;a href=&#34;https://en.wikipedia.org/wiki/Read_(biology)&#34;&gt;reads&lt;/a&gt; that were mapped to the reference genome. Finally, we use statistical tests to determine if the level of change is big enough to be considered as a DEG (differentially expressed gene).&lt;/p&gt;</description>
    </item>
    <item>
      <title>Calculate SVD by hand (and decompose Spongebob)</title>
      <link>/post/svd/</link>
      <pubDate>Mon, 22 Nov 2021 21:18:26 -0600</pubDate>
      <guid>/post/svd/</guid>
      <description>&lt;p&gt;In my &lt;a href=&#34;/post/pca1/&#34;&gt;previous post&lt;/a&gt;, I have manually implemented PCA by finding the eigenvectors and eigenvalues of a covariance matrix. In this post, let&amp;rsquo;s try to perform PCA using a different approach called Singular Value Decomposition. Then we are going to decompose SPONGEBOB!&lt;/p&gt;&#xA;&lt;p&gt;Note: you might find this &lt;a href=&#34;/post/pca1/&#34;&gt;post&lt;/a&gt; to be useful, if you are new to PCA.&#xA; &lt;br&gt;&#xA; &lt;br&gt;&#xA; &lt;br&gt;&#xA; &lt;br&gt;&#xA; &lt;/p&gt;&#xA;&lt;h2 id=&#34;algorithm&#34;&gt;Algorithm&lt;/h2&gt;&#xA;&lt;p&gt;Again, we are going to use the same dataset we have used before.&lt;/p&gt;&#xA;&lt;p&gt;$$&#xA;\mathbf{ M} =&#xA;\begin{bmatrix}&#xA;1 &amp;amp; 0 \\&#xA;0 &amp;amp; 1 \\&#xA;-1 &amp;amp; -1&#xA;\end{bmatrix}&#xA;$$&lt;/p&gt;</description>
    </item>
    <item>
      <title>Dive into Bayesian statistics (5): Intro to PyMC3</title>
      <link>/post/bayesian5/</link>
      <pubDate>Mon, 22 Nov 2021 20:19:26 -0600</pubDate>
      <guid>/post/bayesian5/</guid>
      <description>&lt;p&gt;In &lt;a href=&#34;/post/bayesian3/&#34;&gt;our previous post&lt;/a&gt;, we manually implemented the Markov Chain Monte Carlo (MCMC) algorithm, specifically Metropolis-Hastings, to draw samples from the posterior distribution. The code isn’t particularly difficult to understand, but it’s also not very intuitive to read or write. Besides the challenges of implementation, algorithm performance (i.e., speed) is a major consideration in more realistic applications. Fortunately, well-optimized tools are available to address these obstacles, namely Stan and PyMC3.&lt;/p&gt;&#xA;&lt;p&gt;Subjectively speaking, Stan is not my cup of tea. I remember spending an entire afternoon trying to install RStan, only to fail. To make matters worse, Stan has its own specialized language, adding another layer of complexity. In contrast, PyMC3 is much easier to install. The documentation and tutorials are well-written, and anyone with a basic understanding of Bayesian statistics should be able to follow them without much trouble. In this post—and future posts—I will stick with PyMC3.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Dive into Bayesian statistics (4): Markov Chain Monte Carlo</title>
      <link>/post/bayesian4/</link>
      <pubDate>Mon, 22 Nov 2021 20:18:26 -0600</pubDate>
      <guid>/post/bayesian4/</guid>
      <description>&lt;p&gt;In the last few posts, we tried three methods (&lt;a href=&#34;/post/bayesian2/&#34;&gt;Integration, Conjugate Prior&lt;/a&gt; and &lt;a href=&#34;/post/bayesian3/&#34;&gt;MCMC&lt;/a&gt; to infer the posterior distribution &lt;code&gt;\(P(\lambda | \text{data})\)&lt;/code&gt;, which gave us&lt;/p&gt;&#xA;&lt;p&gt;&lt;code&gt;$$\lambda \sim \text{Gamma}(\alpha = 20, \beta = 6)$$&lt;/code&gt;&lt;/p&gt;&#xA;&lt;p&gt;In this post, we are going to see 1) how to use Bayesian model to make prediction; 2) the internal relationship between a Poisson distribution, a Gamma distribution and a Negative binomial distribution.&lt;/p&gt;&#xA;&lt;p&gt; &lt;br&gt;&#xA; &#xA; &lt;/p&gt;&#xA;&lt;h2 id=&#34;question&#34;&gt;Question:&lt;/h2&gt;&#xA;&lt;p&gt;Here is the data we have worked so far:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Dive into Bayesian statistics (3): Markov Chain Monte Carlo</title>
      <link>/post/bayesian3/</link>
      <pubDate>Mon, 22 Nov 2021 19:18:26 -0600</pubDate>
      <guid>/post/bayesian3/</guid>
      <description>&lt;p&gt;In this post, I will continue to use the same example that I used before (&lt;a href=&#34;/post/bayesian1/&#34;&gt;Bayesian: MAP&lt;/a&gt; and &lt;a href=&#34;/post/bayesian2/&#34;&gt;Bayesian: solve denominator&lt;/a&gt;. Also, it will be very helpful to first understand accept-reject sampling that I discussed in &lt;a href=&#34;/post/monte_carlo/&#34;&gt;this post&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt; &lt;br&gt;&#xA; &lt;br&gt;&#xA; &lt;/p&gt;&#xA;&lt;p&gt;Now let&amp;rsquo;s get started!&lt;/p&gt;&#xA;&lt;p&gt;As we discussed at the end of &lt;a href=&#34;/post/bayesian2/&#34;&gt;this post&lt;/a&gt;, solving the denominator is a non-trivial work, especially when you have many parameters to estimate. One way to overcome this obstacle is to use a method called Markov Chain Monte Carlo (MCMC).&lt;/p&gt;</description>
    </item>
    <item>
      <title>Dive into Bayesian statistics (2): Solve the nasty denominator!</title>
      <link>/post/bayesian2/</link>
      <pubDate>Mon, 22 Nov 2021 18:18:26 -0600</pubDate>
      <guid>/post/bayesian2/</guid>
      <description>&lt;p&gt;In the &lt;a href=&#34;/post/bayesian1/&#34;&gt;last post&lt;/a&gt;, we tried to use a Bayesian framework to model the number of visitors per hour. After concatenating a Poisson distribution with a Gamma prior, we get something like:&#xA;$$&#xA;P(\lambda | \text{data}) = c \cdot \lambda^{19} e^{-6\lambda}&#xA;$$&lt;/p&gt;&#xA;&lt;p&gt;Since we are interested to find &lt;code&gt;\(\lambda_0\)&lt;/code&gt; that gives the maximum value of &lt;code&gt;\(P(\lambda | \text{data})\)&lt;/code&gt; (a.k.a. &lt;strong&gt;Maximum A Posteriori&lt;/strong&gt;), we don&amp;rsquo;t need to worry too much about a constant &lt;code&gt;\(c\)&lt;/code&gt;. But in this post, we are going to solve &lt;code&gt;\(c\)&lt;/code&gt;, and consolidate our understanding of Bayesian inference.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Dive into Bayesian statistics (1): Maximum A Posteriori</title>
      <link>/post/bayesian1/</link>
      <pubDate>Mon, 22 Nov 2021 17:18:26 -0600</pubDate>
      <guid>/post/bayesian1/</guid>
      <description>&lt;p&gt;Before you read this post, I assume you are already familiar with basic probability theories, maximum likelihood estimation and bayes theorem. I encourage you to read my previous post that discussed &lt;a href=&#34;/post/mle/&#34;&gt;MLE&lt;/a&gt;, and we are going to use the same dataset in this post.&lt;/p&gt;&#xA;&lt;p&gt;Okay, let&amp;rsquo;s get started.&lt;/p&gt;&#xA;&lt;p&gt; &lt;br&gt;&#xA; &#xA; &lt;br&gt;&#xA; &lt;/p&gt;&#xA;&lt;h2 id=&#34;1-bayes-theorem&#34;&gt;1. Bayes theorem&lt;/h2&gt;&#xA;&lt;p&gt;In inferential statistics, our goal is to &lt;strong&gt;infer the population parameters&lt;/strong&gt;. That is, we observe the data, and from the data we guess the most likely population parameters. There are, in general, two ways to approach this goal.&lt;/p&gt;</description>
    </item>
    <item>
      <title>How to draw sample from a generic distribution?</title>
      <link>/post/monte_carlo/</link>
      <pubDate>Mon, 22 Nov 2021 16:18:26 -0600</pubDate>
      <guid>/post/monte_carlo/</guid>
      <description>&lt;p&gt;In this post, I am going to show two methods to draw samples from a generic distribution.&lt;/p&gt;&#xA;&lt;p&gt;But before we get started, we should define what do I mean generic distribution. Here is one example:&lt;/p&gt;&#xA;&lt;p&gt;$$&#xA;f(x)= \begin{cases}&#xA;0 &amp;amp; \text{if &lt;code&gt;\(x\)&lt;/code&gt; &amp;lt; 0} \\&#xA;c \cdot \sqrt{x} &amp;amp; \text{if  $0 &amp;lt; x &amp;lt; 1 $} \\&#xA;0 &amp;amp; \text{if &lt;code&gt;\(x &amp;gt; 1\)&lt;/code&gt;} \end{cases}&#xA;$$&lt;/p&gt;&#xA;&lt;p&gt;First, let&amp;rsquo;s take a look at the probability density function (pdf):&lt;br&gt;&#xA; &lt;/p&gt;</description>
    </item>
    <item>
      <title>Maximum likelihood estimation</title>
      <link>/post/mle/</link>
      <pubDate>Mon, 22 Nov 2021 00:00:00 +0000</pubDate>
      <guid>/post/mle/</guid>
      <description>&lt;p&gt;In this post, I will show you &lt;strong&gt;THE&lt;/strong&gt; most important technique in inferential statistics: Maximum Likelihood Estimation (MLE).&lt;/p&gt;&#xA;&lt;p&gt; &lt;br&gt;&#xA; &lt;/p&gt;&#xA;&lt;h2 id=&#34;1-some-data-to-work-with&#34;&gt;1. Some data to work with&lt;/h2&gt;&#xA;&lt;p&gt;Before we get started, let&amp;rsquo;s see what type of problem could be solved using MLE.&lt;/p&gt;&#xA;&lt;p&gt;For example, I recorded the number of visitors of this website each hour from 8:00 am - 12:00 am (&lt;strong&gt;p.s.&lt;/strong&gt; off course this is fake data, and I am probably too optimistic), and I hope to have a model that can accurately describe my data, and well as making predictions.  Here is my data:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Calculate PCA by hand (via eigen-decomposition)</title>
      <link>/post/pca1/</link>
      <pubDate>Sun, 21 Nov 2021 00:00:00 +0000</pubDate>
      <guid>/post/pca1/</guid>
      <description>&lt;p&gt;In this blog post, I will calculate PCA step-by-step (via eigen-decomposition).&lt;/p&gt;&#xA;&lt;p&gt;But before we dive deep into PCA, there are two prerequisite concepts we need to understand:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Variance/Covariance&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Find eigenvectors and eigenvalues&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;If you already familiar those two concepts, feel free to skip those sections.&lt;/p&gt;&#xA;&lt;p&gt; &lt;br&gt;&#xA; &lt;br&gt;&#xA; &lt;br&gt;&#xA; &lt;/p&gt;&#xA;&lt;h2 id=&#34;prerequisite-1-variancecovariance&#34;&gt;Prerequisite 1: Variance/Covariance&lt;/h2&gt;&#xA;&lt;h3 id=&#34;variance&#34;&gt;Variance&lt;/h3&gt;&#xA;&lt;p&gt;Variance measures how far a set of numbers is spread out from their average value. The sample variance is defined as:&lt;/p&gt;</description>
    </item>
    <item>
      <title>About</title>
      <link>/about/</link>
      <pubDate>Thu, 05 May 2016 21:48:51 -0700</pubDate>
      <guid>/about/</guid>
      <description>&lt;p&gt;My name is Taotao Tan, and I am a Ph.D. student in Computational Biology at Baylor College of Medicine. My research interests span a diverse range of topics, including statistics, deep learning, genetics, and functional genomics.&lt;/p&gt;&#xA;&lt;h3 id=&#34;contact&#34;&gt;Contact:&lt;/h3&gt;&#xA;&lt;p&gt;Twitter/X: &lt;a href=&#34;https://x.com/doubleTaoTan&#34;&gt;@doubleTaoTan&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Github: &lt;a href=&#34;https://github.com/JasonTan-code&#34;&gt;@JasonTan-code&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Category</title>
      <link>/category/</link>
      <pubDate>Thu, 05 May 2016 21:48:51 -0700</pubDate>
      <guid>/category/</guid>
      <description>&lt;h3 id=&#34;linear-algebra&#34;&gt;Linear algebra&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;/post/pca1/&#34;&gt;Calculate PCA by hand (via eigen-decomposition)&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;/post/svd/&#34;&gt;Calculate SVD by hand (and decompose Spongebob)&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt; &#xA; &lt;/p&gt;&#xA;&lt;h3 id=&#34;bayesian-network&#34;&gt;Bayesian Network&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;/post/bayesian_network/&#34;&gt;An Intuitive Explanation of Bayesian Network&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt; &#xA; &lt;/p&gt;&#xA;&lt;h3 id=&#34;inferential-statistics&#34;&gt;Inferential Statistics&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;/post/mle/&#34;&gt;Maximum likelihood estimation&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;/post/gene_exp1/&#34;&gt;Model the Gene Expression (1): A GLM framework&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;/post/gene_exp2/&#34;&gt;Model the Gene Expression (2): Likelihood Ratio Test&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt; &#xA; &lt;/p&gt;&#xA;&lt;h3 id=&#34;bayesian-statistics&#34;&gt;Bayesian Statistics&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;/post/bayesian1/&#34;&gt;Dive into Bayesian statistics (1): Maximum A Posteriori&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;/post/bayesian2/&#34;&gt;Dive into Bayesian statistics (2): Solve the nasty denominator!&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;/post/bayesian3/&#34;&gt;Dive into Bayesian statistics (3): Markov Chain Monte Carlo&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;/post/bayesian4/&#34;&gt;Dive into Bayesian statistics (4): Posterior predictive distribution&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;/post/bayesian5/&#34;&gt;Dive into Bayesian statistics (5): Intro to PyMC3&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt; &#xA; &lt;/p&gt;</description>
    </item>
  </channel>
</rss>
