<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on A Hugo website</title>
    <link>/post/</link>
    <description>Recent content in Posts on A Hugo website</description>
    <generator>Hugo</generator>
    <language>en-US</language>
    <lastBuildDate>Sun, 16 Feb 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="/post/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Genome-wide association studies</title>
      <link>/post/gwas/</link>
      <pubDate>Sun, 16 Feb 2025 00:00:00 +0000</pubDate>
      <guid>/post/gwas/</guid>
      <description>&lt;h2 id=&#34;variants-trait-association&#34;&gt;Variants-trait association&lt;/h2&gt;&#xA;&lt;p&gt;The core objective of genetic studies is to identify which genetic variants contribute to disease risk. While establishing direct causation is challenging, we can detect statistical associations between genetic variants and traits by analyzing large-scale genomic data.&lt;/p&gt;&#xA;&lt;p&gt;Large biobanks (such as the UK Biobank) have collected genomic data from hundreds of thousands of samples. To study variant-trait associations, one approach is to apply linear or logistic regression for each genetic variant, treating genotypes as independent variables and the trait as the dependent variable.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Linkage Disequilibrium Score Regression</title>
      <link>/post/ldsc/</link>
      <pubDate>Sun, 05 Jan 2025 00:00:00 +0000</pubDate>
      <guid>/post/ldsc/</guid>
      <description>&lt;h2 id=&#34;ldsc-derivation&#34;&gt;LDSC derivation&lt;/h2&gt;&#xA;&lt;p&gt;We discussed how to perform &lt;a href=&#34;/post/gwas/&#34;&gt;GWAS with scaled genotypes &amp;amp; phenotype&lt;/a&gt;. In this blog post, I present an important piece of result: Linkage Disequilibrium Score Regression (LDSC)&lt;/p&gt;&#xA;&lt;p&gt;LDSC was proposed in &lt;a href=&#34;https://www.nature.com/articles/ng.3211&#34;&gt;this&lt;/a&gt; landmark paper, in which it described how LD affect the probability of a variant being significant. Under infinitesimal model, LDSC states &lt;code&gt;\(\mathbb{E}[\chi_j^2] = \frac{Nh^2}{M} l_j + 1\)&lt;/code&gt;, where &lt;code&gt;\(l_j \equiv \sum_{k = 1}^M r_{jk}^2\)&lt;/code&gt; is the LD score. To carry out the derivation, one must treat the effect size as random: &lt;code&gt;\(\lambda_j \sim N(0, \frac{h^2}{M})\)&lt;/code&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Hidden Markov Model (1) - Markov Chain</title>
      <link>/post/hmm1/</link>
      <pubDate>Mon, 29 Nov 2021 00:00:00 +0000</pubDate>
      <guid>/post/hmm1/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;&#xA;&lt;p&gt;This series of blog posts aims to explore the Hidden Markov Model (HMM) due to its broad applications across various fields, including natural language processing, population genetics, finance, and more. Beyond its practical utility, I find HMM particularly fascinating because it bridges multiple disciplines such as probability, linear algebra, machine learning, and computer science. In this post, I will introduce the Markov Chain, which serves as the foundation of HMM. As before, the concepts will be explained through a simple example, with minimal use of complex mathematical notation.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Hidden Markov Model (2) - Forward Backward Propagation</title>
      <link>/post/hmm2/</link>
      <pubDate>Mon, 29 Nov 2021 00:00:00 +0000</pubDate>
      <guid>/post/hmm2/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;&#xA;&lt;p&gt;This post will include a few sections:&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Introducing HMM, and demonstrate how it different from the Markov Chain&lt;/li&gt;&#xA;&lt;li&gt;Introducing an exhaustive method to infer the hidden state&lt;/li&gt;&#xA;&lt;li&gt;Introducing forward-backward propagation as an improvement&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;The example is from &lt;a href=&#34;https://www.youtube.com/watch?v=VBs8FYsZIN4&#34;&gt;Dr.Xiaole Liu&amp;rsquo;s Youtube channel&lt;/a&gt;, and I highly recommend you to check out her video if you want to develop intuition of HMM rather than get killed by notations. Also, you may want to review &lt;a href=&#34;https://en.wikipedia.org/wiki/Conditional_independence&#34;&gt;conditional independence&lt;/a&gt; before you start reading, since it will be very frequently used later in this post.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Maximum likelihood estimation</title>
      <link>/post/mle/</link>
      <pubDate>Mon, 22 Nov 2021 00:00:00 +0000</pubDate>
      <guid>/post/mle/</guid>
      <description>&lt;p&gt;In this post, I will show you &lt;strong&gt;THE&lt;/strong&gt; most important technique in inferential statistics: Maximum Likelihood Estimation (MLE).&lt;/p&gt;&#xA;&lt;p&gt; &lt;br&gt;&#xA; &lt;/p&gt;&#xA;&lt;h2 id=&#34;1-some-data-to-work-with&#34;&gt;1. Some data to work with&lt;/h2&gt;&#xA;&lt;p&gt;Before we get started, let&amp;rsquo;s see what type of problem could be solved using MLE.&lt;/p&gt;&#xA;&lt;p&gt;For example, I recorded the number of visitors of this website each hour from 8:00 am - 12:00 am (&lt;strong&gt;p.s.&lt;/strong&gt; off course this is fake data, and I am probably too optimistic), and I hope to have a model that can accurately describe my data, and well as making predictions.  Here is my data:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Calculate PCA by hand (via eigen-decomposition)</title>
      <link>/post/pca1/</link>
      <pubDate>Sun, 21 Nov 2021 00:00:00 +0000</pubDate>
      <guid>/post/pca1/</guid>
      <description>&lt;p&gt;In this blog post, I will calculate PCA step-by-step (via eigen-decomposition).&lt;/p&gt;&#xA;&lt;p&gt;But before we dive deep into PCA, there are two prerequisite concepts we need to understand:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Variance/Covariance&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Find eigenvectors and eigenvalues&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;If you already familiar those two concepts, feel free to skip those sections.&lt;/p&gt;&#xA;&lt;p&gt; &lt;br&gt;&#xA; &lt;br&gt;&#xA; &lt;br&gt;&#xA; &lt;/p&gt;&#xA;&lt;h2 id=&#34;prerequisite-1-variancecovariance&#34;&gt;Prerequisite 1: Variance/Covariance&lt;/h2&gt;&#xA;&lt;h3 id=&#34;variance&#34;&gt;Variance&lt;/h3&gt;&#xA;&lt;p&gt;Variance measures how far a set of numbers is spread out from their average value. The sample variance is defined as:&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
